# Retrieval-Augmented Generation (RAG) System with Dynamic Web Corpus

## Project Introduction

This repository hosts an Information Retrieval project focused on building a Retrieval-Augmented Generation (RAG) system. Unlike traditional IR systems that operate on static datasets, our system dynamically creates its own document corpus by crawling the web. This custom-built corpus is then leveraged to answer natural language queries by a Language Model (LLM), providing contextually relevant and accurate responses.

The project demonstrates the end-to-end process of web content acquisition, processing, indexing, and retrieval, combining classic IR principles with modern LLM capabilities.

## Project Structure

The repository is organized into three main directories, reflecting the logical flow of our system:

```bash
.
├── crawling/
├── scraping/
└── retrieving/
```

### `crawling/`

This directory contains the core logic for our web crawler. Its responsibilities include:
* **URL Management:** Managing the queue of URLs to be visited (`URL Frontier`).
* **HTTP Requests:** Handling fetching web pages from the internet.
* **Politeness:** Ensuring adherence to `robot.txt` directives and implementing time delays between requests to avoid overloading web servers.
* **Robustness:** Incorporating basic heuristics to prevent common "spider traps" or infinite crawling loops (e.g., depth limits, detection of repetitive URL patterns).

### `scraping/`

This directory focuses on processing the raw HTML content fetched by the crawler. Its key functionalities are:
* **HTML Parsing:** Analyzing the downloaded HTML to extract its structural components and raw text.
* **Main Content Extraction:** Identifying and isolating the primary textual content of a web page, effectively filtering out "boilerplate" elements such as advertisements, navigation links, headers, and footers.
* **Link Extraction:** Identifying and extracting all internal and external hyperlinks present on a page to feed back into the `crawling` module's URL frontier.
* **Duplicate and Near-Duplicate Detection:** Implementing mechanisms (e.g., checksumming for exact duplicates and simplified fingerprinting for near-duplicates) to avoid processing and indexing redundant content.

### `retrieving/`

This directory houses the components of our RAG system, which utilize the cleaned documents generated by the `scraping` module.
* **Document Indexing:** Processing the extracted main content for efficient storage and retrieval.
* **Hybrid Retriever:** This is a core component that combines two methods for finding relevant information:
    * **Vectorial Retrieval:** Uses a pre-trained embedding model to convert document chunks and queries into dense vector representations. Retrieval is based on vector similarity search (e.g., cosine similarity).
    * **Keyword-Based Retrieval:** Implements a mechanism similar to an inverted index to calculate a score based on traditional keyword matching and frequency within documents, aligning with classic IR techniques.
    * The final retrieval process combines scores from both vectorial and keyword-based methods to provide a comprehensive ranking of relevant document chunks.
* **Generator (LLM Integration):** Integrates a Language Model that takes the user's natural language query and the retrieved relevant document chunks as context to synthesize a coherent and informative answer.

## Key Features

* **Dynamic Corpus Generation:** Builds its own dataset through controlled web crawling.
* **Intelligent Content Extraction:** Focuses on relevant main content, minimizing noise.
* **Duplicate Content Handling:** Avoids redundancy by detecting and managing duplicate and near-duplicate pages.
* **Hybrid Retrieval System:** Combines the strengths of modern vectorial search with traditional keyword-based methods for robust retrieval.
* **Natural Language Querying:** Utilizes RAG to answer free-form text queries effectively.

## Technologies Used

* **Python:** The primary programming language for the entire project.
* **`requests`:** For making HTTP/S requests during web crawling.
* **`BeautifulSoup` / `lxml`:** For efficient HTML parsing and DOM navigation.
* **`nltk` / `spaCy`:** (Potentially) for text preprocessing, tokenization, and stemming.
* **`Hugging Face Transformers`:** For integrating pre-trained embedding models and potentially smaller LLMs.

## Setup and Installation

To set up the project locally, follow these steps:

1.  **Clone the Repository:**
    ```bash
    git clone https://github.com/TommasoMoro03/IR-group-project.git](https://github.com/TommasoMoro03/IR-group-project.git)
    cd IR-group-project
    ```
2.  **Create a Virtual Environment (Recommended):**
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows: `venv\Scripts\activate`
    ```
3.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
    (You will need to create a `requirements.txt` file listing all necessary libraries like `requests`, `beautifulsoup4`, `lxml`, `transformers`, `torch`/`tensorflow`, etc.)

## Usage

Instructions on how to run the crawler, process the data, and interact with the RAG system will be provided here.

1.  **Configure Crawler:** (e.g., specify seed URLs, domain limits)
2.  **Run Crawler:** `python crawling/main_crawler.py`
3.  **Process Scraped Data:** `python scraping/process_data.py`
4.  **Build Retriever Index:** `python retrieving/build_index.py`
5.  **Start RAG System:** `python retrieving/run_rag.py`

## Team Members

* Tommaso Moro
* Margherita Necchi
* Ester De Giosa

## License

[Choose a license, e.g., MIT License, Apache License 2.0]